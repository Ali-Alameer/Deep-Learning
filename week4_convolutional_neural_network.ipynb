{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Alameer/Deep-Learning/blob/main/week4_convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkfgjYcE9R1z"
      },
      "source": [
        "Convolutional Layers: These layers perform convolution operations on the input images to extract relevant features. The Conv2D layers use small square filters and a rectified linear unit (relu) activation function to learn and detect different patterns in the images.\n",
        "\n",
        "Pooling Layer: The MaxPooling2D layer downsamples the feature maps obtained from the convolutional layers by selecting the maximum value within each pooling region. This helps reduce the spatial dimensions while retaining the most important features.\n",
        "\n",
        "Flatten Layer: This layer transforms the multidimensional feature maps into a flat vector. It allows the subsequent fully connected layers to process the extracted features.\n",
        "\n",
        "Dense Layers: These fully connected layers use the extracted features to classify the input images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFM-1YGS87Xe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "# Reshape the data for CNN\n",
        "X_train = X_train.reshape(-1, 8, 8, 1) / 16.0  # Normalize the pixel values to the range [0, 1] and reshape for CNN\n",
        "X_val = X_val.reshape(-1, 8, 8, 1) / 16.0\n",
        "X_test = X_test.reshape(-1, 8, 8, 1) / 16.0\n",
        "\n",
        "# Convert the target labels to categorical format\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create a convolutional neural network (CNN)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9AaUD4J-3mP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = y_pred.argmax(axis=1)\n",
        "y_test_labels = y_test.argmax(axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Precision:\", precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP1mNT5U-xTR"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(np.arange(1, len(history.history['accuracy']) + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTeA9ky6BOm-"
      },
      "source": [
        "Let's have a look at our input dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtF1PW7V9nyQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Get the number of samples and image dimensions\n",
        "num_samples, img_height, img_width = X_train.shape[0], digits.images[0].shape[0], digits.images[0].shape[1]\n",
        "\n",
        "# Visualize the first few training images\n",
        "num_images_to_visualize = 5\n",
        "fig, axes = plt.subplots(1, num_images_to_visualize, figsize=(12, 4))\n",
        "\n",
        "for i in range(num_images_to_visualize):\n",
        "    axes[i].imshow(digits.images[i], cmap='gray')\n",
        "\n",
        "# Display image size and shape information\n",
        "print(\"Number of training samples:\", num_samples)\n",
        "print(\"Image size: {} x {}\".format(img_height, img_width))\n",
        "print(\"Image shape:\", digits.images[0].shape)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNjCcrVYCiOr"
      },
      "source": [
        "# Visualize the label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOgNkidDBh-B"
      },
      "outputs": [],
      "source": [
        "label_counts = np.sum(y_train, axis=0)\n",
        "\n",
        "# Visualize the label distribution\n",
        "labels = range(10)\n",
        "plt.bar(labels, label_counts)\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.xticks(labels)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNuAbRVRVBdOLaugLZZhGUF",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
