{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Alameer/Deep-Learning/blob/main/week7_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# Train YOLOv7 on a Custom Dataset\n",
        "\n",
        "This workshop is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu.\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "To train our detector we take the following steps:\n",
        "\n",
        "* Install YOLOv7 dependencies\n",
        "* Load custom dataset from Roboflow in YOLOv7 format\n",
        "* Run YOLOv7 training\n",
        "* Evaluate YOLOv7 performance\n",
        "* Run YOLOv7 inference on test images\n",
        "* OPTIONAL: Deployment\n",
        "* OPTIONAL: Active Learning\n",
        "\n",
        "\n",
        "### Preparing a Custom Dataset\n",
        "\n",
        "In this workshop, we will utilise a previosly collected dataset with the aim to detect pig heads and rears to then quantify interactions [Head and Rear Detection](https://salford.figshare.com/articles/dataset/Automated_detection_and_quantification_of_contact_behaviour_in_pigs_using_deep_learning/21346767).\n",
        "\n",
        "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "#Install Dependencies\n",
        "\n",
        "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGJspIl90XhV"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD-uPyQ_2jiN"
      },
      "outputs": [],
      "source": [
        "# Download YOLOv7 repository and install requirements\n",
        "\n",
        "# !git clone https://github.com/WongKinYiu/yolov7\n",
        "# %cd yolov7\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# current version of YOLOv7 is not compatible with pytorch>1.12.1 and numpy>1.20.1\n",
        "# until the appropriate changes get made to the main repository, we will be using a fork containing the patched code\n",
        "# you can track the progress here: https://github.com/roboflow/notebooks/issues/27\n",
        "!git clone https://github.com/SkalskiP/yolov7.git\n",
        "%cd yolov7\n",
        "!git checkout fix/problems_associated_with_the_latest_versions_of_pytorch_and_numpy\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXdgPlv82xI7"
      },
      "source": [
        "\n",
        "- Download the dataset from the provided link: [Automated Detection and Quantification of Contact Behaviour in Pigs Dataset](https://salford.figshare.com/articles/dataset/Automated_detection_and_quantification_of_contact_behaviour_in_pigs_using_deep_learning/21346767)\n",
        "- Extract the downloaded dataset and locate the folder that contains the pig parts detection with any type of annotation. This folder will be used for further processing.\n",
        "- Create an account with Roboflow if you don't have one already. Roboflow is a platform that helps in preparing and managing datasets for computer vision tasks.\n",
        "- Once logged in to Roboflow, create a new object detection project by selecting the appropriate option from the project creation menu.\n",
        "- Follow the instructions to set up the project and select the folder containing the dataset that you extracted in step 2. Upload the dataset to Roboflow and wait for the upload process to complete.\n",
        "- After the dataset is uploaded, you can choose to split the dataset into training, validating and testing sets (code will not work if not all three are available) based on your requirements. Follow the provided options and instructions to perform the split and wait for the process to complete.\n",
        "- Once the dataset is split, you will be presented with several steps. Click \"Continue\" on each step without applying any preprocessing or augmentation. These steps are optional and can be used to preprocess and augment the dataset if required. For this exercise, we will skip them.\n",
        "- Finally, click \"Generate Dataset\" to proceed. On the following page, you will find an option to export the dataset. Choose the export format suitable for YOLOv7 and select \"Show Download Code\" to obtain the code needed for downloading the dataset.\n",
        "- Copy the provided code, which may include commands or scripts, and use it to download the exported dataset. Ensure that you have the necessary dependencies, such as PyTorch, installed to run YOLOv7 on the downloaded dataset.\n",
        "By following these steps, students can obtain an object detection dataset in the appropriate format for running YOLOv7 using the Roboflow platform and the provided dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtJ24mPlyF-S"
      },
      "source": [
        "# Download Correctly Formatted Custom Data\n",
        "\n",
        "Next, we'll download our dataset in the right format. Use the `YOLOv7 PyTorch` export. Note that this model requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us and saves it in the correct spot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovKgrVN8ygdW"
      },
      "outputs": [],
      "source": [
        "# REPLACE with your custom code snippet generated above\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YOUR API KEY\")\n",
        "project = rf.workspace(\"YOUR-WORKSPACE\").project(\"YOUR-PROJECT\")\n",
        "dataset = project.version(3).download(\"yolov7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHfT9gEiBsBd"
      },
      "source": [
        "# Begin Custom Training\n",
        "\n",
        "We're ready to start custom training.\n",
        "\n",
        "NOTE: We will only modify one of the YOLOv7 training defaults in our example: `epochs`. We will adjust from 300 to 100 epochs in our example for speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUbmy674bhpD"
      },
      "outputs": [],
      "source": [
        "# download COCO starting checkpoint\n",
        "%cd /content/yolov7\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iqOPKjr22mL"
      },
      "outputs": [],
      "source": [
        "# run this cell to begin training; for real implemenation increase number of epochs\n",
        "%cd /content/yolov7\n",
        "!python train.py --batch 16 --epochs 1 --data {dataset.location}/data.yaml --weights 'yolov7_training.pt' --device 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W0MpUaTCJro"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "We can evaluate the performance of our custom training using the provided evalution script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4cfnLtTCIce"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "!python detect.py --weights runs/train/exp/weights/best.pt --conf 0.1 --source {dataset.location}/test/images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AGhNOSSHY4_"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "i = 0\n",
        "limit = 10000 # max images to print\n",
        "for imageName in glob.glob('/content/yolov7/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    if i < limit:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")\n",
        "    i = i + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jn4kCtgKiGO"
      },
      "source": [
        "# OPTIONAL: Deployment\n",
        "\n",
        "To deploy, you'll need to export your weights and save them to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWOok8abrCsL"
      },
      "outputs": [],
      "source": [
        "# optional, zip to download weights and results locally\n",
        "\n",
        "!zip -r export.zip runs/detect\n",
        "!zip -r export.zip runs/train/exp/weights/best.pt\n",
        "!zip export.zip runs/train/exp/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41PvE5gKhYw"
      },
      "source": [
        "# OPTIONAL: Active Learning Example\n",
        "\n",
        "Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n",
        "\n",
        "To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcINqQS7Kt3-"
      },
      "outputs": [],
      "source": [
        "# # setup access to your workspace\n",
        "# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n",
        "# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n",
        "# model = inference_project.version(1).model\n",
        "\n",
        "# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
        "\n",
        "# print(\"inference reference point: \", inference_project)\n",
        "# print(\"upload destination: \", upload_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEl1NVE3LSD_"
      },
      "outputs": [],
      "source": [
        "# # example upload: if prediction is below a given confidence threshold, upload it\n",
        "\n",
        "# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n",
        "\n",
        "# for prediction in predictions:                                  # predictions list to loop through\n",
        "#   if(prediction['confidence'] * 100 >= confidence_interval[0] and\n",
        "#           prediction['confidence'] * 100 <= confidence_interval[1]):\n",
        "\n",
        "#           # upload on success!\n",
        "#           print(' >> image uploaded!')\n",
        "#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
